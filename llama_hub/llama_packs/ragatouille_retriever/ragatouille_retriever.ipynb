{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "183222bc-eb44-4385-8bf0-2dc55263ec2f",
   "metadata": {},
   "source": [
    "# RAGatouille Retriever Llama Pack \n",
    "\n",
    "RAGatouille is a [cool library](https://github.com/bclavie/RAGatouille) that lets you use e.g. ColBERT and other SOTA retrieval models in your RAG pipeline. You can use it to either run inference on ColBERT, or use it to train/fine-tune models.\n",
    "\n",
    "This LlamaPack shows you an easy way to bundle RAGatouille into your RAG pipeline. We use RAGatouille to index a corpus of documents (by default using colbertv2.0), and then we combine it with LlamaIndex query modules to synthesize an answer with an LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98a4e842-47fa-4403-a2d2-7047dd2bddea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option: if developing with the llama_hub package\n",
    "from llama_hub.llama_packs.ragatouille_retriever.base import RAGatouilleRetrieverPack\n",
    "\n",
    "# Option: download_llama_pack\n",
    "# from llama_index.llama_pack import download_llama_pack\n",
    "\n",
    "# RAGatouilleRetrieverPack = download_llama_pack(\n",
    "#     \"RAGatouilleRetrieverPack\",\n",
    "#     \"./ragatouille_pack\",\n",
    "#     skip_load=True,\n",
    "#     # leave the below line commented out if using the notebook on main\n",
    "#     # llama_hub_url=\"https://raw.githubusercontent.com/run-llama/llama-hub/jerry/add_llm_compiler_pack/llama_hub\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bbc8e8-f9e3-4d5c-8c36-309cac65969a",
   "metadata": {},
   "source": [
    "## Load Documents\n",
    "\n",
    "Here we load the ColBERTv2 paper: https://arxiv.org/pdf/2112.01488.pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8cde968-5d4f-42c3-b2c6-dd97b4656901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-01-04 15:15:56--  https://arxiv.org/pdf/2112.01488.pdf\n",
      "Resolving arxiv.org (arxiv.org)... 151.101.131.42, 151.101.195.42, 151.101.3.42, ...\n",
      "Connecting to arxiv.org (arxiv.org)|151.101.131.42|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1081592 (1.0M) [application/pdf]\n",
      "Saving to: ‘colbertv2.pdf’\n",
      "\n",
      "colbertv2.pdf       100%[===================>]   1.03M  --.-KB/s    in 0.06s   \n",
      "\n",
      "2024-01-04 15:15:56 (17.5 MB/s) - ‘colbertv2.pdf’ saved [1081592/1081592]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget \"https://arxiv.org/pdf/2112.01488.pdf\" -O colbertv2.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc41a62d-7d3e-431c-9d8a-08447df14b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import SimpleDirectoryReader\n",
    "from llama_index.llms import OpenAI\n",
    "\n",
    "reader = SimpleDirectoryReader(input_files=[\"colbertv2.pdf\"])\n",
    "docs = reader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43d767af-5f4e-4dbf-9cfe-8df13df3b9e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jerryliu/Programming/llama-hub/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "artifact.metadata: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1.63k/1.63k [00:00<00:00, 3.22MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[Jan 04, 15:19:14] #> Creating directory .ragatouille/colbert/indexes/my_index \n",
      "\n",
      "\n",
      "#> Starting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 743/743 [00:00<00:00, 2.58MB/s]\n",
      "pytorch_model.bin: 100%|██████████| 438M/438M [00:10<00:00, 41.0MB/s] \n",
      "tokenizer_config.json: 100%|██████████| 405/405 [00:00<00:00, 1.48MB/s]\n",
      "vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 10.3MB/s]\n",
      "tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 31.2MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 112/112 [00:00<00:00, 148kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Jan 04, 15:19:28] Loading segmented_maxsim_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jerryliu/Programming/llama-hub/.venv/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py:125: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]/Users/jerryliu/Programming/llama-hub/.venv/lib/python3.10/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Jan 04, 15:19:34] [0] \t\t #> Encoding 129 passages..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [00:04<00:08,  4.01s/it]/Users/jerryliu/Programming/llama-hub/.venv/lib/python3.10/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "100%|██████████| 3/3 [00:07<00:00,  2.40s/it]\n",
      "WARNING clustering 21076 points to 2048 centroids: please provide at least 79872 training points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Jan 04, 15:19:42] [0] \t\t avg_doclen_est = 171.97674560546875 \t len(local_sample) = 129\n",
      "[Jan 04, 15:19:42] [0] \t\t Creating 2,048 partitions.\n",
      "[Jan 04, 15:19:42] [0] \t\t *Estimated* 22,185 embeddings.\n",
      "[Jan 04, 15:19:42] [0] \t\t #> Saving the indexing plan to .ragatouille/colbert/indexes/my_index/plan.json ..\n",
      "Clustering 21076 points in 128D to 2048 clusters, redo 1 times, 20 iterations\n",
      "  Preprocessing in 0.00 s\n",
      "[0.034, 0.035, 0.034, 0.031, 0.032, 0.035, 0.034, 0.032, 0.033, 0.035, 0.031, 0.033, 0.031, 0.036, 0.034, 0.034, 0.032, 0.033, 0.032, 0.032, 0.033, 0.035, 0.033, 0.033, 0.031, 0.034, 0.034, 0.032, 0.033, 0.036, 0.032, 0.038, 0.034, 0.033, 0.035, 0.029, 0.034, 0.032, 0.033, 0.039, 0.035, 0.034, 0.031, 0.034, 0.034, 0.032, 0.032, 0.037, 0.035, 0.035, 0.03, 0.034, 0.035, 0.035, 0.032, 0.035, 0.039, 0.033, 0.036, 0.033, 0.033, 0.037, 0.034, 0.034, 0.035, 0.036, 0.034, 0.033, 0.031, 0.033, 0.034, 0.034, 0.034, 0.033, 0.036, 0.035, 0.035, 0.036, 0.037, 0.037, 0.037, 0.032, 0.034, 0.037, 0.033, 0.034, 0.034, 0.034, 0.032, 0.038, 0.034, 0.034, 0.035, 0.035, 0.034, 0.033, 0.036, 0.032, 0.034, 0.033, 0.036, 0.037, 0.034, 0.034, 0.036, 0.032, 0.03, 0.032, 0.034, 0.034, 0.035, 0.036, 0.034, 0.032, 0.033, 0.033, 0.034, 0.033, 0.033, 0.036, 0.034, 0.034, 0.034, 0.035, 0.032, 0.038, 0.031, 0.031]\n",
      "[Jan 04, 15:19:42] [0] \t\t #> Encoding 129 passages..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      " 33%|███▎      | 1/3 [00:03<00:06,  3.18s/it]\u001b[A\n",
      "100%|██████████| 3/3 [00:06<00:00,  2.15s/it]\u001b[A\n",
      "1it [00:06,  6.53s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00, 3890.82it/s]\n",
      "100%|██████████| 2048/2048 [00:00<00:00, 356354.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Jan 04, 15:19:49] #> Optimizing IVF to store map from centroids to list of pids..\n",
      "[Jan 04, 15:19:49] #> Building the emb2pid mapping..\n",
      "[Jan 04, 15:19:49] len(emb2pid) = 22185\n",
      "[Jan 04, 15:19:49] #> Saved optimized IVF to .ragatouille/colbert/indexes/my_index/ivf.pid.pt\n",
      "\n",
      "#> Joined...\n",
      "Done indexing!\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "RetrieverQueryEngine.__init__() got an unexpected keyword argument 'service_context'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ragatouille_pack \u001b[38;5;241m=\u001b[39m \u001b[43mRAGatouilleRetrieverPack\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-3.5-turbo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Programming/llama-hub/llama_hub/llama_packs/ragatouille_retriever/base.py:69\u001b[0m, in \u001b[0;36mRAGatouilleRetrieverPack.__init__\u001b[0;34m(self, documents, model_name, index_name, llm, index_path)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdocuments \u001b[38;5;241m=\u001b[39m documents\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm \u001b[38;5;241m=\u001b[39m llm \u001b[38;5;129;01mor\u001b[39;00m OpenAI(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-3.5-turbo\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 69\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquery_engine \u001b[38;5;241m=\u001b[39m \u001b[43mRetrieverQueryEngine\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcustom_retriever\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43mservice_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mServiceContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_defaults\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: RetrieverQueryEngine.__init__() got an unexpected keyword argument 'service_context'"
     ]
    }
   ],
   "source": [
    "index_name = \"my_index\"\n",
    "ragatouille_pack = RAGatouilleRetrieverPack(\n",
    "    docs,\n",
    "    llm=OpenAI(model=\"gpt-3.5-turbo\"),\n",
    "    index_name=index_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d30b0f-c4ef-4fcd-83b9-00a04f3e2309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the custom retriever (which just uses RAG.search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c8b06b-a495-4a91-821d-9b38f9669a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = ragatouille_pack.get_modules()[\"retriever\"]\n",
    "nodes = retriever.retrieve(\"How does ColBERTv2 compare with SPLADEv2?\")\n",
    "\n",
    "# even lower-level\n",
    "RAG = ragatouille_pack.get_modules()[\"RAG\"]\n",
    "results = RAG.search(\"How does ColBERTv2 compare with SPLADEv2?\", index_name=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985406f6-beb5-49cc-b1b7-6943c4e91201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run pack e2e, which includes the full query engine with OpenAI LLMs\n",
    "response = ragatouille_pack.run(\"How does ColBERTv2 compare with SPLADEv2?\")\n",
    "print(str(response))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_hub",
   "language": "python",
   "name": "llama_hub"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
